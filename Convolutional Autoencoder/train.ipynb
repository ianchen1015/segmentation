{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tkuanlun350/Tensorflow-SegNet/blob/master/model.py\n",
    "https://ithelp.ithome.com.tw/articles/10188326"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pythpm :  3.6.4 (v3.6.4:d48ecebad5, Dec 18 2017, 21:07:28) \n",
      "[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)]\n",
      "tensorflow :  1.7.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print('pythpm : ',sys.version)\n",
    "print('tensorflow : ',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paremeters\n",
    "img_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 256\n",
    "# load training data\n",
    "def next_batch(batch_size):\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk('./data/x'):\n",
    "        for name in files:\n",
    "            filenames.append(os.path.join(root, name).split('/')[-1])\n",
    "\n",
    "    data_shape = (batch_size, img_size, img_size)\n",
    "    X = np.zeros(data_shape)\n",
    "    Y = np.zeros(data_shape)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        f = random.choice(filenames)\n",
    "        img = np.array(cv2.imread('./data/x/' + f, 0))\n",
    "        img2 = np.array(cv2.imread('./data/y/' + f, 0))\n",
    "        X[i, :, :] = img\n",
    "        Y[i, :, :] = img2\n",
    "    \n",
    "    X = X.reshape(batch_size, img_size*img_size)\n",
    "    Y = Y.reshape(batch_size, img_size*img_size)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variable(shape, name):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial, name)\n",
    "\n",
    "def bias_variable(shape, name):\n",
    "    initial = tf.constant(0.1, shape = shape)\n",
    "    return tf.Variable(initial, name)\n",
    "'''\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding = 'SAME')\n",
    "'''\n",
    "\n",
    "def conv2d_layer(x, W_shape, b_shape, name, padding='SAME'):\n",
    "    W = weight_variable(W_shape, name+'_W')\n",
    "    b = bias_variable([b_shape], name+'_b')\n",
    "    return tf.nn.relu(tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=padding) + b)\n",
    "'''\n",
    "def deconv2d_(x, W, output_shape):\n",
    "    return tf.nn.conv2d_transpose(x, W, output_shape, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    " '''   \n",
    "def deconv_layer(x, W_shape, b_shape, name, padding='SAME'):\n",
    "    W = weight_variable(W_shape, name+'_W')\n",
    "    b = bias_variable([b_shape], name+'_b')\n",
    "    x_shape = tf.shape(x)\n",
    "    out_shape = tf.stack([x_shape[0], x_shape[1], x_shape[2], W_shape[2]])\n",
    "    return tf.nn.conv2d_transpose(x, W, out_shape, [1, 1, 1, 1], padding=padding) + b\n",
    "\n",
    "def max_pool_2x2_layer(x):\n",
    "    #_, argmax = tf.nn.max_pool_with_argmax(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding = 'SAME')\n",
    "    pool = tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "    return pool\n",
    "\n",
    "def max_unpool_2x2_layer(x, shape): # input shape\n",
    "    inference = tf.image.resize_nearest_neighbor(x, tf.stack([shape[1]*2, shape[2]*2]))\n",
    "    return inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input layer shape : (?, 256, 256, 1)\n",
      "code layer shape : (?, 64, 64, 128)\n",
      "output layer shape : (?, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.float32, shape = [None, 256*256], name='x')\n",
    "y = tf.placeholder(tf.float32, shape = [None, 256*256], name='y')\n",
    "x_origin = tf.reshape(x, [-1, 256, 256, 1])\n",
    "y_origin = tf.reshape(y, [-1, 256, 256, 1])\n",
    "\n",
    "# conv1 256,1 > 256,64\n",
    "'''\n",
    "W_e_conv1 = weight_variable([5, 5, 1, 64], \"w_e_conv1\") # filter, channel, features\n",
    "b_e_conv1 = bias_variable([64], \"b_e_conv1\")\n",
    "h_e_conv1 = tf.nn.relu(tf.add(conv2d(x_origin, W_e_conv1), b_e_conv1))\n",
    "'''\n",
    "conv_1 = conv2d_layer(x_origin, [5, 5, 1, 64], 64, \"conv_1\", padding='SAME')\n",
    "# pool1 256,64 > 128,64\n",
    "pool_1 = max_pool_2x2_layer(conv_1)\n",
    "\n",
    "# conv2 128,64 > 128,128\n",
    "'''\n",
    "W_e_conv2 = weight_variable([5, 5, 64, 128], \"w_e_conv2\")\n",
    "b_e_conv2 = bias_variable([128], \"b_e_conv2\")\n",
    "h_e_conv2 = tf.nn.relu(tf.add(conv2d(h_e_pool1, W_e_conv2), b_e_conv2))\n",
    "'''\n",
    "conv_2 = conv2d_layer(pool_1, [5, 5, 64, 128], 128, \"conv_2\", padding='SAME')\n",
    "# pool2 128,128 > 64,128\n",
    "pool_2 = max_pool_2x2_layer(conv_2)\n",
    "\n",
    "# code 64,128\n",
    "code_layer = pool_2\n",
    "\n",
    "# deconv1 64,128 > 64,64\n",
    "'''\n",
    "W_d_conv1 = weight_variable([5, 5, 64, 128], \"w_d_conv1\")\n",
    "output_shape_d_conv1 = tf.stack([tf.shape(x)[0], 64, 64, 64])\n",
    "h_d_conv1 = tf.nn.sigmoid(deconv2d(code_layer, W_d_conv1, output_shape_d_conv1))\n",
    "'''\n",
    "deconv_1 = deconv_layer(code_layer, [5, 5, 64, 128], 64, 'deconv_1', padding='SAME')\n",
    "# unpool1 64,64 > 128,64\n",
    "unpool_1 = max_unpool_2x2_layer(deconv_1, [-1, 64, 64, 64]) # input size\n",
    "\n",
    "# deconv2 128,64 > 128,1\n",
    "'''\n",
    "W_d_conv2 = weight_variable([5, 5, 1, 64], \"w_d_conv2\")\n",
    "output_shape_d_conv2 = tf.stack([tf.shape(x)[0], 128, 128, 1])\n",
    "h_d_conv2 = tf.nn.sigmoid(deconv2d(h_d_pool1, W_d_conv2, output_shape_d_conv2))\n",
    "'''\n",
    "deconv_2 = deconv_layer(unpool_1, [5, 5, 1, 64], 1, 'deconv_2', padding='SAME')\n",
    "# unpool 2 128,1 > 256,1\n",
    "unpool_2 = max_unpool_2x2_layer(deconv_2, [-1, 128, 128, 1])\n",
    "\n",
    "x_reconstruct = unpool_2\n",
    "\n",
    "result = tf.sigmoid(x_reconstruct, name='result')\n",
    "\n",
    "print(\"input layer shape : %s\" % x_origin.get_shape())\n",
    "print(\"code layer shape : %s\" % code_layer.get_shape())\n",
    "print(\"output layer shape : %s\" % result.get_shape())\n",
    "\n",
    "# optimizer\n",
    "with tf.name_scope('loss'):\n",
    "    #cost = tf.reduce_mean(tf.pow(y_origin - result, 2))\n",
    "    cost = tf.sqrt(tf.reduce_mean(tf.square(y_origin - result)))\n",
    "    tf.summary.scalar('loss', cost)\n",
    "optimizer = tf.train.AdamOptimizer(1e-4).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss 61.6927\n",
      "model saved\n"
     ]
    }
   ],
   "source": [
    "# GPU config\n",
    "#gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "#config = tf.ConfigProto(gpu_options=gpu_options)\n",
    "\n",
    "#sess = tf.Session(config = config)\n",
    "#sess = tf.InteractiveSession()\n",
    "w1 = tf.placeholder(\"float\", name=\"w1\")\n",
    "\n",
    "batch_size = 60\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # logs\n",
    "    merged = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    for i in range(5000):\n",
    "        batch_x, batch_y = next_batch(batch_size)\n",
    "        if i%50 == 0: # loss logs\n",
    "            rs = sess.run(merged,feed_dict={x:batch_x, y:batch_y})\n",
    "            writer.add_summary(rs, i)\n",
    "        if i%100 == 0: # print loss\n",
    "            print(\"step %d, loss %g\"%(i, cost.eval(feed_dict={x:batch_x, y:batch_y})))\n",
    "        if i%5000 == 0: # save\n",
    "            saver.save(sess, 'save/model.ckpt')\n",
    "            print('model saved')\n",
    "\n",
    "        optimizer.run(feed_dict={x:batch_x, y:batch_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
